# Curriculum Phase 2: Introduce real motor lag
#
# Resume from Phase 1, gradually adapt to the real drone's motor_tau.
# Real drone: mass=0.774, motor_tau=0.033
# Start with hover init since motor lag makes spin-up harder.
#
# Usage:
#   python scripts/train.py --config configs/curriculum/phase2_motor_lag.yaml \
#       --resume models/PPO_3000000/   # <-- Phase 1 best model directory
#
# Expected: brief performance dip then recovery within ~2-3M steps.

env:
  max_episode_steps: 1000
  vec_env:
    seed: 1
    scene_id: 0
    num_envs: 4
    num_threads: 2
    render: false

  quadrotor_dynamics:
    mass: 0.774
    arm_l: 0.125
    motor_omega_min: 150.0
    motor_omega_max: 2800.0
    motor_tau: 0.033
    thrust_map: [1.562522e-6, 0.0, 0.0]
    kappa: 0.022
    omega_max: [10.0, 10.0, 4.0]

  motor_init: hover

  domain_randomization:
    enabled: false
    mass_range: [0.6, 0.9]
    motor_tau_range: [0.02, 0.05]

  action_history_len: 5

  custom_reward:
    enabled: true
    x_goal: [0.0, 0.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
    act_goal: [0.0, 0.0, 0.0, 0.0]
    rew_state_weight: [1.0, 1.0, 1.5, 0.2, 0.2, 0.2, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]
    rew_act_weight: [0.001, 0.001, 0.001, 0.001]
    rew_exponential: true

ppo:
  learning_rate: 3.0e-4
  n_steps: 1024
  batch_size: 256
  n_epochs: 8
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.001
  vf_coef: 0.5
  max_grad_norm: 0.5
  policy_kwargs:
    net_arch:
      pi: [256, 256]
      vf: [256, 256]

training:
  total_timesteps: 3000000
  log_interval: 1
  save_interval: 100000
  eval_freq: 100000
  seed: 0
  normalize_obs: true
  normalize_reward: true
  record_episode_statistics_deque_size: 100

evaluation:
  n_episodes: 5
  max_episode_steps: 1000
  render: false
  deterministic: true

paths:
  log_dir: logs
  save_dir: saved
  checkpoint_path: saved/ppo_drone_final.zip
  plot_dir: val_plots
